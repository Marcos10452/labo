{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/improve-your-model-performance-with-bayesian-optimization-hyperparameter-tuning-4dbd7fe25b62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import randint, uniform\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def parameter_over_iterations(model_result):\n",
    "  '''\n",
    "  This function is generating a subplots with the hyperparameter values for each iteration and the overall performance score.\n",
    "  The performance score is the difference between the best performing model and the worst performing model\n",
    "  \n",
    "  model_result: CV object\n",
    "  '''\n",
    "  param_list = list(model_result.cv_results_['params'][0].keys())\n",
    "  max_col_plot = 2\n",
    "  row_plot =int(np.ceil((len(param_list) + 1)/max_col_plot))\n",
    "  fig, axs = plt.subplots(nrows=row_plot, ncols=np.min((max_col_plot, (len(param_list) + 1))), figsize=(30,12))\n",
    "  for i, ax in enumerate(axs.flatten()):\n",
    "    if i == len(param_list):\n",
    "      break\n",
    "    par = param_list[i]\n",
    "    param_val = list()\n",
    "    for par_dict in model_result.cv_results_['params']:\n",
    "      param_val.append(par_dict[par])\n",
    "    sns.barplot(y=param_val, x=np.arange(len(param_val)), ax=ax)\n",
    "    ax.set_title(par)\n",
    "  dt = pd.DataFrame({key:val for key,  val in model_result.cv_results_.items() if key.startswith('split')})\n",
    "  mean_metric = dt.mean(axis=1)\n",
    "  sns.barplot(y=(mean_metric.values + abs(np.min(mean_metric.values))), x=np.arange(len(mean_metric) ), ax=axs.flatten()[i])\n",
    "  axs.flatten()[i].set_title('overall metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "In this section, we will see the results of using GridSearch to select the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1c07679680e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m               'max_features': np.arange(0.4,1,0.3), 'n_estimators': np.arange(40,160,60)}\n\u001b[1;32m      3\u001b[0m \u001b[0mgsearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mparameter_over_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsearch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "param_test = {'max_depth':range(5,15,5), 'min_samples_split':range(200,800,300), 'learning_rate': np.arange(0.05,0.55,0.25), 'subsample': np.arange(0.4,1,0.4),\n",
    "              'max_features': np.arange(0.4,1,0.3), 'n_estimators': np.arange(40,160,60)}\n",
    "gsearch = GridSearchCV(estimator = GradientBoostingRegressor(random_state=10),param_grid = param_test, scoring='neg_mean_absolute_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch.fit(X_train,y_train)\n",
    "parameter_over_iterations(gsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "RandomSearch should lead to better results than GridSearch, even though typically it is not able to reach the global optima of the unknown function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distrib = {'max_depth':randint(5,15), 'min_samples_split':randint(200,800), 'learning_rate': uniform(loc=0.05, scale=0.50), 'subsample': uniform(loc=0.4, scale=0.6),\n",
    "              'max_features': uniform(loc=0.4, scale=0.6), 'n_estimators': randint(40,160)}\n",
    "rsearch = RandomizedSearchCV(estimator = GradientBoostingRegressor(random_state=10), \n",
    "param_distributions = param_distrib, scoring='neg_mean_absolute_error',n_jobs=4, n_iter=64,iid=False, cv=5)\n",
    "rsearch.fit(X_train,y_train)\n",
    "parameter_over_iterations(rsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization\n",
    "\n",
    "Now is time to test the Bayesian optimization algorithm to tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "optimizer_kwargs = {'acq_func_kwargs':{\"xi\": 10, \"kappa\": 10}}\n",
    "space  = {'max_depth':Integer(5, 15),\n",
    "          'learning_rate':Real(0.05, 0.55, \"uniform\"),\n",
    "          'min_samples_split':Integer(200, 800),\n",
    "          'subsample': Real(0.4, 1, \"uniform\"),\n",
    "          'max_features': Real(0.4, 1, \"uniform\"),\n",
    "          'n_estimators': Integer(40, 160)}\n",
    "bsearch = BayesSearchCV(estimator = GradientBoostingRegressor(random_state=10), \n",
    "search_spaces = space, scoring='neg_mean_absolute_error',n_jobs=4, n_iter=64,iid=False, cv=5, optimizer_kwargs=optimizer_kwargs)\n",
    "bsearch.fit(X_train,y_train)\n",
    "parameter_over_iterations(bsearch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a31e2ed7ed067d9015dca975175f652aec64d99e2d93e1f8a4dccdf393d8c0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
